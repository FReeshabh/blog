{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"My first machine learning hackathon. Finishing #2 on both challenges \"\n",
    "> \"My first machine learning hackathon, and my almost win\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [notebook, hackathon, machine-learning]\n",
    "- image: images/codeml/codeML.jpeg\n",
    "- hide: false"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges in this Hackathon\n",
    "\n",
    "This past weekend I participated in the CodeML hackathon, which was my first machine learning hackathon experience. Given this blog post, is about a machine learning hackathon, I feel that it's quite apt to write this blog in Jupyter Notebooks. \n",
    "\n",
    "There were 6 challenges hosted on Kaggle, and they were:\n",
    "\n",
    "1. Celestial Body Clustering\n",
    "2. Anomaly Detection\n",
    "3. Reddit comments classification\n",
    "4. Sentiment Analysis\n",
    "5. Image Classification\n",
    "6. Easy Imagenet Classification (Or as I later found out, it was not really Imagenet)\n",
    "\n",
    "\n",
    "I tried my hand at Challenge 5, and Challenge 6. And this blog post documents my experience with these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You don't need your own GPU or expensive hardware! I run a used $100 Thinkpad that I got off eBay. Google Colab, Kaggle, and Paperspace (there's probably more I'm failing to list here) provide free or inexpensive GPU instances you can use for training your deep models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 5 - Image classification challenge\n",
    "\n",
    "I found out that that this challenge was testing out the CIFAR-10 dataset, which I somehow didn't realize at the time. Nevertheless, I scored a 0.94550 on the private leaderboard with the categorization accuracy metric.\n",
    "\n",
    "## Using Transfer learning\n",
    "People who have already been actively taking part in Kaggle competitions, or any other machine learning competitions should see this as a no brainer, however for newbies who often try to train their models from scratch in such compeitions might miss on something that can give them a **huge jump in performance**(they are also particularly popular within Image recognition tasks).\n",
    "\n",
    "### A *very* brief guide to transfer learning\n",
    "I don't plan to teach you transfer learning in this blog however before I send you running off onto the many different resources to learn transfer learning, I'll explain it a bit.\n",
    "\n",
    "**This deep learning technique enables developers to harness a neural network used for one task and apply it to another domain** -Nvidia's blog\n",
    "\n",
    "That's basically it in a nutshell. Let's say we have a machine learning model trained to classify images within the domain of vehicles - cars, motorcycles, etc. With a bit of tweaking and *a little training* of the last few layers (or just one) of the network, we can repurpose the model to fit our own task.\n",
    "\n",
    "We can view how these various pretrained models perform for certain tasks. A popular benchmark to see how these pretrained models would perform on the ImageNet dataset. Here's [Stanford's DAWNBench](https://dawn.cs.stanford.edu/benchmark/). We can see a lot of these top models for image recognition are ResNets.\n",
    "\n",
    "#### ResNet and it's *magicks*\n",
    "ResNet is one of the most popular pre-trained models used for image recognition, and it comes in the sizes of 18 layers, 34 layers, 50 layers, and 101 layers (there's also a 152 model). The paper for [Deep Residual Learning for Image Recognition\n",
    " can be found here.](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "I don't want to give the impression that transfer learning is only a valid technique for image recognition, in the past few years we have found it emerge it into other fields of machine learnning too. Here's a [talk by Thomas Wolf on using transfer learning for NLP using Hugging Face](https://youtu.be/t86G11tfVNw)\n",
    "\n",
    "You can learn more about ResNet from [fastbook](https://github.com/fastai/fastbook/blob/master/14_resnet.ipynb), [d2l.ai book](https://d2l.ai/chapter_convolutional-modern/resnet.html), or the multiple other resources you can find on the internet.\n",
    "\n",
    "I used transfer learning with a Resnet 50 model for this image recognition task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Learning rate finder is a great tool too. And so are discriminative learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 6\n",
    "I used the same approach for this challenge as I did for Challenge 5, and while it initially gave me an edge over other competitors, I realize in hindsight my step 1 should probably have to try to realize what the dataset was. My score was around 68% categorization accuracy on the task, and 70-71% categorization accuracy with a late submission, while the winning submission had a categorization accuracy of 80%+."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Try half point precision for faster training: to_fp_16(), and to_native_fp_16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what happened?\n",
    "\n",
    "An ***Adversarial Attack***\n",
    "\n",
    "### What is an Adversarial attack?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An adversarial attack is when the dataset is designed to intentionally fool the model. In my model, this wasn't a possibility I had considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts and takeaways\n",
    "Despite not winning the challenges, but still being one of the top teams. I defintely learned a lot, and here are some of my takeaways from the hackathon.\n",
    "\n",
    "\n",
    "1. **Inspect the dataset**:\n",
    "\n",
    "    Not considering an adversarial attack, or recognizing a popular dataset was a mistake on my end. So inspecting the data thouroughly is quite important.\n",
    "    \n",
    "2. **Consider using transfer learning**:\n",
    "\n",
    "     Transfer learning is usually a way to get state of the art results while giving results faster than training a model from scratch.\n",
    "\n",
    "\n",
    "3. **Data Visualization and plot confusions**:\n",
    "\n",
    "    Check what the model usually messes up on, any data visualizations to aid your understanding of your model's results will be helpful - Confusion Matrix, most confused list, anything else.\n",
    "        \n",
    "4. **Revise your model**:\n",
    "    \n",
    "    After constructing a baseline model. Revise your model!\n",
    "    \n",
    "    Try different paramaters. KFold validation. Try a deeper model like ResNet101 (as a last resort)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
